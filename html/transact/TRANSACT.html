<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>transact.TRANSACT API documentation</title>
<meta name="description" content="&lt;h1&gt; &lt;b&gt;TRANSACT&lt;/b&gt;: &lt;b&gt;T&lt;/b&gt;umor &lt;b&gt;R&lt;/b&gt;esponse &lt;b&gt;A&lt;/b&gt;ssessment by &lt;b&gt;N&lt;/b&gt;on-linear &lt;b&gt;S&lt;/b&gt;ubspace
&lt;b&gt;A&lt;/b&gt;lignment of &lt;b&gt;C&lt;/b&gt;ell-lines and â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>transact.TRANSACT</code></h1>
</header>
<section id="section-intro">
<h1> <b>TRANSACT</b>: <b>T</b>umor <b>R</b>esponse <b>A</b>ssessment by <b>N</b>on-linear <b>S</b>ubspace
<b>A</b>lignment of <b>C</b>ell-lines and <b>T</b>umors </h1>
<p>@author: Soufiane Mourragui <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#115;&#111;&#117;&#102;&#105;&#97;&#110;&#101;&#46;&#109;&#111;&#117;&#114;&#114;&#97;&#103;&#117;&#105;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#115;&#111;&#117;&#102;&#105;&#97;&#110;&#101;&#46;&#109;&#111;&#117;&#114;&#114;&#97;&#103;&#117;&#105;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a></p>
<p>Method supporting the design of drug response models that translate from pre-clinical models to tumors. The complete
methodological details can be found in our <a href="https://www.biorxiv.org/content/10.1101/2020.06.29.177139v3">
pre-print</a>.
<br/><br/></p>
<h2 id="example">Example</h2>
<pre><code>::
import numpy as np
from transact.TRANSACT import TRANSACT

# Generate data
n_source = 100
n_target = 200
n_features = 500

X_source = np.random.normal(size=(n_source, n_features))
y_source = X_source.dot(np.random.normal(size=(n_features)))
X_target = np.random.normal(size=(n_target, n_features))


# Create a TRANSACT instance
clf = TRANSACT(
    kernel='rbf',
    kernel_params={'gamma':1/np.sqrt(n_features)},
    n_components={'source': 20, 'target':40},
    n_jobs=1,
    verbose=1
)

# Compute consensus features
clf.fit(
    X_source,
    X_target,
    n_pv=10,
    step=100,
    with_interpolation=True
)
::
</code></pre>
<h2 id="notes">Notes</h2>
<p>TRANSACT required Python 3.6 or higher, and the following packages are required: scikit-learn, numpy, scipy, joblib.
<br/>
Please relate any issue on the GitHub, or contact me (s.mourragui@nki.nl).</p>
<h2 id="references">References</h2>
<p>[1] Mourragui et al 2021, Predicting clinical drug response from model systems by non-linear subspace-based transfer
learning, Biorxiv.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34; &lt;h1&gt; &lt;b&gt;TRANSACT&lt;/b&gt;: &lt;b&gt;T&lt;/b&gt;umor &lt;b&gt;R&lt;/b&gt;esponse &lt;b&gt;A&lt;/b&gt;ssessment by &lt;b&gt;N&lt;/b&gt;on-linear &lt;b&gt;S&lt;/b&gt;ubspace
&lt;b&gt;A&lt;/b&gt;lignment of &lt;b&gt;C&lt;/b&gt;ell-lines and &lt;b&gt;T&lt;/b&gt;umors &lt;/h1&gt;

@author: Soufiane Mourragui &lt;soufiane.mourragui@gmail.com&gt;

Method supporting the design of drug response models that translate from pre-clinical models to tumors. The complete
methodological details can be found in our &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.06.29.177139v3&#34;&gt;
pre-print&lt;/a&gt;.
&lt;br/&gt;&lt;br/&gt;


Example
-------
    ::
    import numpy as np
    from transact.TRANSACT import TRANSACT

    # Generate data
    n_source = 100
    n_target = 200
    n_features = 500

    X_source = np.random.normal(size=(n_source, n_features))
    y_source = X_source.dot(np.random.normal(size=(n_features)))
    X_target = np.random.normal(size=(n_target, n_features))


    # Create a TRANSACT instance
    clf = TRANSACT(
        kernel=&#39;rbf&#39;,
        kernel_params={&#39;gamma&#39;:1/np.sqrt(n_features)},
        n_components={&#39;source&#39;: 20, &#39;target&#39;:40},
        n_jobs=1,
        verbose=1
    )

    # Compute consensus features
    clf.fit(
        X_source,
        X_target,
        n_pv=10,
        step=100,
        with_interpolation=True
    )
    ::
    
Notes
-------

TRANSACT required Python 3.6 or higher, and the following packages are required: scikit-learn, numpy, scipy, joblib.
&lt;br/&gt;
Please relate any issue on the GitHub, or contact me (s.mourragui@nki.nl).


References
-------
[1] Mourragui et al 2021, Predicting clinical drug response from model systems by non-linear subspace-based transfer
learning, Biorxiv.


&#34;&#34;&#34;


import numpy as np
import scipy
from joblib import Parallel, delayed

from sklearn.model_selection import GridSearchCV, KFold
from sklearn.linear_model import Ridge, ElasticNet, Lasso, LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.base import clone

from transact.pv_computation import PVComputation
from transact.interpolation import Interpolation
from transact.kernel_computer import KernelComputer


class TRANSACT:
    &#34;&#34;&#34;
    TRANSACT is a package designed to adapt predictors of drug response from pre-clinical models to the clinic.
    &lt;br/&gt;&lt;br/&gt;
    This class contains all the tasks and sub-routines required for training the domain adaptation framework, i.e.:
    &lt;ul&gt;
        &lt;li&gt; Kernel PCA decomposition on source and target independently.
         &lt;li&gt; Kernel principal components comparison.
         &lt;li&gt; Computation of Principal Vectors (PVs).
         &lt;li&gt; Interpolation between source and target PVs and extraction of Consensus Features (CFs).
         &lt;li&gt; Out-of-sample extension: project new dataset onto the consensus features.
    &lt;/ul&gt;
    &#34;&#34;&#34;

    def __init__(self,
                kernel=&#39;linear&#39;,
                kernel_params=None,
                n_components=None,
                n_pv=None,
                method=&#39;two-stage&#39;,
                step=100,
                n_jobs=1,
                verbose=False):
        &#34;&#34;&#34;
        Parameters
        ----------
        kernel : str, default to &#39;linear&#39;
            Name of the kernel to be used in the algorithm. Has to be compliant with
            &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.kernel_metrics.html#sklearn.metrics.pairwise.kernel_metrics&#34;&gt;
            scikit-learn kernel&lt;/a&gt;, e.g., &#34;rbf&#34;, &#34;polynomial&#34;, &#34;laplacian&#34;, &#34;linear&#34;, ...

        kernel_params : dict, default to None
            Parameters of the kernel (degree for polynomial kernel, gamma for RBF).
            Naming has to be compliant with scikit-learn, e.g., {&#34;gamma&#34;: 0.0005}.

        n_components : int or dict, default to None
            Number of components for kernel PCA.
            &lt;br/&gt; If int, then indicates the same number of components for source and target.
            &lt;br/&gt; If dict, then must be of the form {&#39;source&#39;:int, &#39;target&#39;:int}.

        n_pv : int, default to None
            Number of principal vectors.

        method : str, default to &#39;two-stage&#39;
            Method used for computing the principal vectors. Only &#39;two-stage&#39; has been implemented.

        step: int, default to 100
            Number of interpolation steps.

        n_jobs: int, default to 1
            Number of concurrent threads to use for tasks that can be parallelized.

        verbose: bool or int, default to False
            Degree of verbosity in joblib routines.
        &#34;&#34;&#34;

        self.kernel = kernel
        self.kernel_params_ = kernel_params or {}
        self.kernel_values_ = KernelComputer(self.kernel, self.kernel_params_)

        self.source_data_ = None
        self.target_data_ = None

        self.is_fitted = False

        self.n_components = n_components
        self.n_pv = n_pv
        self.method = method
        self.step = step

        self.predictive_clf = None

        self.n_jobs = n_jobs
        self.verbose = verbose


    def fit(self,
            source_data,
            target_data,
            n_components=None,
            n_pv=None,
            method=&#39;two-stage&#39;,
            step=100,
            with_interpolation=True,
            left_center=True):

        &#34;&#34;&#34;
        Compute the Consensus Features (CFs) onto which predictive models can be trained.
        &lt;br/&gt; Specifically:
        &lt;ul&gt;
            &lt;li&gt; Compute the kernel matrices.
            &lt;li&gt; Compute the cosine similarity matrix.
            &lt;li&gt; Compute principal vectors.
            &lt;li&gt; Interpolate between the PVs.
            &lt;li&gt; Find optimal interpolation time.
        &lt;/ul&gt;

        Parameters
        ----------
        source_data : np.ndarray, dtype=float
            Source data, matrix with samples in the rows, i.e. shape (n_source_samples, n_features).
            &lt;br./&gt; pandas.DataFrame are supported.

        target_data : np.ndarray, dtype=float
            Source data, matrix with samples in the rows, i.e. shape (n_target_samples, n_features).
            &lt;br./&gt; pandas.DataFrame are supported.
            &lt;br/&gt;&lt;b&gt;WARNING&lt;/b&gt;: features need to be ordered in the same way as in source_data.

        n_components: int, default to None
            Number of components. If not set here or in __init__, then use the maximum number of principal components
            possible for source and target.

        n_pv: int, default to None
            Number of Principal Vectors. If not set here or in __init__, then maximum number of PV will be computed.

        method : str, default to &#39;two-stage&#39;
            Method used for computing the principal vectors. Only &#39;two-stage&#39; has been implemented.

        step: int, default to 100
            Number of interpolation steps.

        with_interpolation: bool, default to True
            Bool indicating whether interpolation shall also be fitted. Useful for just computing PV
            prior to null distribution fitting (and choose of PV number).

        left_center: bool, default to True
            Bool indicating whether the output should be mean-centered, i.e. whether source and target
            consensus features values (or PVs if no interpolation) must have an independent mean-centering.

        Returns
        -------
        self : TRANSACT
            Fitted instance.
        &#34;&#34;&#34;

        # Save parameters
        self.source_data_ = source_data
        self.target_data_ = target_data
        self.method = method or self.method
        self.n_components = n_components or self.n_components
        self.n_pv = n_pv or self.n_pv
        self.step = step or self.step
        self.left_center = left_center

        # Compute kernel values
        self.kernel_values_.fit(source_data, target_data, center=False)

        # Compute principal vectors
        self.principal_vectors_ = PVComputation(self.kernel, self.kernel_params_)
        self.principal_vectors_.fit(self.source_data_,
                                    self.target_data_,
                                    method=self.method,
                                    n_components=self.n_components,
                                    n_pv=self.n_pv)

        # Stop here if interpolation should not be computed.
        if not with_interpolation:
            return self

        # Set up interpolation scheme
        self.interpolation_ = Interpolation(self.kernel, self.kernel_params_)
        self.interpolation_.fit(self.principal_vectors_, self.kernel_values_)

        # Compute optimal interpolation time
        self._compute_optimal_time(step=self.step, left_center=self.left_center)

        self.is_fitted = True

        return self


    def null_distribution_pv_similarity(self, method=&#39;gene_shuffling&#39;, n_iter=100):
        &#34;&#34;&#34;
        Generate a null distribution for the PV similarity function:
        &lt;ul&gt;
            &lt;li&gt; Gene shuffling: genes get shuffled in source to destroy any structure existing
            at the gene-level while preserving the sample structure. PV get recomputed and 
            similarity is saved.
        &lt;/ul&gt;

        Parameters
        ----------
        method : string, default to gene_shuffling
            Method used for generating the null distribution.
            Only method developped: gene_shuffling

        n_iter: int, default to 100
            Number of iterations

        Returns
        -------
        np.ndarray, dtype=float, shape (n_iter, n_pv)
            Array containing the distribution of similarity after shuffling. Each row
            contains the values of one shuffling across PVs.
        &#34;&#34;&#34;

        if method.lower() == &#39;gene_shuffling&#39;:
            null_method = self._gene_shuffling
        else:
            raise NotImplementedError(&#39;%s is not a proper method for generating null distribution&#39;%(method))

        null_distribution = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)\
                                    (delayed(null_method)() for _ in range(n_iter))

        return np.array(null_distribution)


    def _gene_shuffling(self):
        perm = np.random.permutation(self.source_data_.shape[1])
        pv = PVComputation(self.kernel, self.kernel_params_)
        pv.fit(self.source_data_[:,perm],
            self.target_data_,
            method=self.method,
            n_components=self.n_components,
            n_pv=self.n_pv)

        return np.cos(pv.canonical_angles)


    def fit_predictor(self, X, y, alpha_values=None, l1_ratio=0.5):
        &#34;&#34;&#34;
        Project X on consensus features and train a predictor of drug response.

        Parameters
        ----------
        X : np.ndarray of shape (n_samples, n_features), dtype=float
            Dataset to project. Features should be ordered in same way as in source_data
            and target_data.

        y : np.ndarray of shape (n_samples, 1), dtype=float
            Output to predict

        Returns
        -------
        &#34;&#34;&#34;
        self.alpha_values = alpha_values if alpha_values is not None else np.logspace(-10,5,34)
        self.l1_ratio_values = [0.2, 0.4, 0.5, 0.6, 0.8, 0.9, 1.]
        param_grid ={
            &#39;regression__alpha&#39;: self.alpha_values,
            &#39;regression__l1_ratio&#39;: self.l1_ratio_values
        }

        #Grid search setup
        self.predictive_clf = GridSearchCV(Pipeline([
                                (&#39;scaler&#39;, StandardScaler(with_mean=True, with_std=False)),
                                (&#39;regression&#39;, ElasticNet())
                                ]),\
                                cv=10,
                                n_jobs=self.n_jobs,
                                param_grid=param_grid,
                                verbose=self.verbose,
                                scoring=&#39;neg_mean_squared_error&#39;)
        self.predictive_clf.fit(self.transform(X, center=False), y)

        return self


    def compute_pred_performance(self, X, y, cv=10):
        &#34;&#34;&#34;
        Compute predictive performance of predictive model by cross-validation
        on X and y.

        Parameters
        ----------
        X : np.ndarray of shape (n_samples, n_features), dtype=float
            Dataset to project. Features should be ordered in same way as in source_data
            and target_data.

        Returns
        -------
        np.ndarray of shape (n_samples, n_pv), dtype=float
            Dataset projected on consensus features.
        &#34;&#34;&#34;

        kf = KFold(n_splits=cv, shuffle=True)
        X_projected = self.transform(X)

        if self.predictive_clf is None:
            print(&#39;BEWARE: NOT FITTED INSTANCE&#39;)
            self.fit_predictor(X,y)
        clf = clone(self.predictive_clf)

        y_predicted = np.zeros(X.shape[0])
        for train_index, test_index in kf.split(X_projected):
            clf.fit(X_projected[train_index], y[train_index])
            y_predicted[test_index] = clf.predict(X_projected[test_index])

        return scipy.stats.pearsonr(y_predicted, y)


    def predict(self, X):
        &#34;&#34;&#34;
        Predict the drug response of a set of samples, i.e.:
        &lt;ul&gt;
            &lt;li&gt; Project data on consensus features.
            &lt;li&gt; Use the Elastic Net model to predict based on the consensus features.
        &lt;/ul&gt;

        Parameters
        ----------
        X : np.ndarray, dtype=float
            Dataset to project, of shape (n_samples, n_features). Features should be ordered in same way as
            in source_data and target_data.

        Returns
        -------
        np.ndarray of shape (n_samples, 1), dtype=float
            Predicted drug response values.
        &#34;&#34;&#34;
        return self.predictive_clf.predict(self.transform(X, center=False))


    def transform(self, X, center=False):
        &#34;&#34;&#34;
        Project a dataset X onto the consensus features.

        Parameters
        ----------
        X : np.ndarray, dtype=float
            Dataset to project, of shape (n_samples, n_features). Features should be ordered in same way as
            in source_data and target_data.

        Returns
        -------
        np.ndarray of shape (n_samples, n_pv), dtype=float
            Dataset projected on consensus features.
        &#34;&#34;&#34;
        return self.interpolation_.transform(X, self.optimal_time, center)


    def _compute_optimal_time(self, step=100, left_center=True):
        # Based on Kolmogorov Smirnov statistics, find interpolation time

        # Compute the interpolated values
        interpolated_values = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)\
                            (delayed(self.interpolation_.project_data)(s/step, center=left_center)
                                for s in range(step+1))
        interpolated_values = np.array(interpolated_values).transpose(2,0,1)
        source_interpolated_values = interpolated_values[:,:,:self.source_data_.shape[0]]
        target_interpolated_values = interpolated_values[:,:,self.source_data_.shape[0]:]

        self.optimal_time = []
        self.ks_statistics = []
        self.ks_p_values = []

        # For each PV, find the time when interpolation has the largest overlap.
        for source_pv, target_pv in zip(source_interpolated_values, target_interpolated_values):
            self.ks_statistics.append([])
            for s, t in zip(source_pv, target_pv):
                self.ks_statistics[-1].append(scipy.stats.ks_2samp(s,t))
            self.ks_statistics[-1] = list(zip(*self.ks_statistics[-1]))
            self.ks_p_values.append(self.ks_statistics[-1][-1])
            self.ks_statistics[-1] = self.ks_statistics[-1][0]
            self.optimal_time.append(np.argmin(self.ks_statistics[-1])/step)

        # Save the different statistics
        self.optimal_time = np.array(self.optimal_time) # Optimal tau for each PV.
        self.ks_statistics = np.array(self.ks_statistics) # Computed KS statistics between each PV.
        self.ks_p_values = np.array(self.ks_p_values) # Corresponding p_values.</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="transact.TRANSACT.TRANSACT"><code class="flex name class">
<span>class <span class="ident">TRANSACT</span></span>
<span>(</span><span>kernel='linear', kernel_params=None, n_components=None, n_pv=None, method='two-stage', step=100, n_jobs=1, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>TRANSACT is a package designed to adapt predictors of drug response from pre-clinical models to the clinic.
<br/><br/>
This class contains all the tasks and sub-routines required for training the domain adaptation framework, i.e.:</p>
<ul>
<li> Kernel PCA decomposition on source and target independently.
<li> Kernel principal components comparison.
<li> Computation of Principal Vectors (PVs).
<li> Interpolation between source and target PVs and extraction of Consensus Features (CFs).
<li> Out-of-sample extension: project new dataset onto the consensus features.
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kernel</code></strong> :&ensp;<code>str</code>, default <code>to 'linear'</code></dt>
<dd>Name of the kernel to be used in the algorithm. Has to be compliant with
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.kernel_metrics.html#sklearn.metrics.pairwise.kernel_metrics">
scikit-learn kernel</a>, e.g., "rbf", "polynomial", "laplacian", "linear", &hellip;</dd>
<dt><strong><code>kernel_params</code></strong> :&ensp;<code>dict</code>, default <code>to None</code></dt>
<dd>Parameters of the kernel (degree for polynomial kernel, gamma for RBF).
Naming has to be compliant with scikit-learn, e.g., {"gamma": 0.0005}.</dd>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int</code> or <code>dict</code>, default <code>to None</code></dt>
<dd>Number of components for kernel PCA.
<br/> If int, then indicates the same number of components for source and target.
<br/> If dict, then must be of the form {'source':int, 'target':int}.</dd>
<dt><strong><code>n_pv</code></strong> :&ensp;<code>int</code>, default <code>to None</code></dt>
<dd>Number of principal vectors.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code>, default <code>to 'two-stage'</code></dt>
<dd>Method used for computing the principal vectors. Only 'two-stage' has been implemented.</dd>
<dt><strong><code>step</code></strong> :&ensp;<code>int</code>, default <code>to 100</code></dt>
<dd>Number of interpolation steps.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, default <code>to 1</code></dt>
<dd>Number of concurrent threads to use for tasks that can be parallelized.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code> or <code>int</code>, default <code>to False</code></dt>
<dd>Degree of verbosity in joblib routines.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TRANSACT:
    &#34;&#34;&#34;
    TRANSACT is a package designed to adapt predictors of drug response from pre-clinical models to the clinic.
    &lt;br/&gt;&lt;br/&gt;
    This class contains all the tasks and sub-routines required for training the domain adaptation framework, i.e.:
    &lt;ul&gt;
        &lt;li&gt; Kernel PCA decomposition on source and target independently.
         &lt;li&gt; Kernel principal components comparison.
         &lt;li&gt; Computation of Principal Vectors (PVs).
         &lt;li&gt; Interpolation between source and target PVs and extraction of Consensus Features (CFs).
         &lt;li&gt; Out-of-sample extension: project new dataset onto the consensus features.
    &lt;/ul&gt;
    &#34;&#34;&#34;

    def __init__(self,
                kernel=&#39;linear&#39;,
                kernel_params=None,
                n_components=None,
                n_pv=None,
                method=&#39;two-stage&#39;,
                step=100,
                n_jobs=1,
                verbose=False):
        &#34;&#34;&#34;
        Parameters
        ----------
        kernel : str, default to &#39;linear&#39;
            Name of the kernel to be used in the algorithm. Has to be compliant with
            &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.kernel_metrics.html#sklearn.metrics.pairwise.kernel_metrics&#34;&gt;
            scikit-learn kernel&lt;/a&gt;, e.g., &#34;rbf&#34;, &#34;polynomial&#34;, &#34;laplacian&#34;, &#34;linear&#34;, ...

        kernel_params : dict, default to None
            Parameters of the kernel (degree for polynomial kernel, gamma for RBF).
            Naming has to be compliant with scikit-learn, e.g., {&#34;gamma&#34;: 0.0005}.

        n_components : int or dict, default to None
            Number of components for kernel PCA.
            &lt;br/&gt; If int, then indicates the same number of components for source and target.
            &lt;br/&gt; If dict, then must be of the form {&#39;source&#39;:int, &#39;target&#39;:int}.

        n_pv : int, default to None
            Number of principal vectors.

        method : str, default to &#39;two-stage&#39;
            Method used for computing the principal vectors. Only &#39;two-stage&#39; has been implemented.

        step: int, default to 100
            Number of interpolation steps.

        n_jobs: int, default to 1
            Number of concurrent threads to use for tasks that can be parallelized.

        verbose: bool or int, default to False
            Degree of verbosity in joblib routines.
        &#34;&#34;&#34;

        self.kernel = kernel
        self.kernel_params_ = kernel_params or {}
        self.kernel_values_ = KernelComputer(self.kernel, self.kernel_params_)

        self.source_data_ = None
        self.target_data_ = None

        self.is_fitted = False

        self.n_components = n_components
        self.n_pv = n_pv
        self.method = method
        self.step = step

        self.predictive_clf = None

        self.n_jobs = n_jobs
        self.verbose = verbose


    def fit(self,
            source_data,
            target_data,
            n_components=None,
            n_pv=None,
            method=&#39;two-stage&#39;,
            step=100,
            with_interpolation=True,
            left_center=True):

        &#34;&#34;&#34;
        Compute the Consensus Features (CFs) onto which predictive models can be trained.
        &lt;br/&gt; Specifically:
        &lt;ul&gt;
            &lt;li&gt; Compute the kernel matrices.
            &lt;li&gt; Compute the cosine similarity matrix.
            &lt;li&gt; Compute principal vectors.
            &lt;li&gt; Interpolate between the PVs.
            &lt;li&gt; Find optimal interpolation time.
        &lt;/ul&gt;

        Parameters
        ----------
        source_data : np.ndarray, dtype=float
            Source data, matrix with samples in the rows, i.e. shape (n_source_samples, n_features).
            &lt;br./&gt; pandas.DataFrame are supported.

        target_data : np.ndarray, dtype=float
            Source data, matrix with samples in the rows, i.e. shape (n_target_samples, n_features).
            &lt;br./&gt; pandas.DataFrame are supported.
            &lt;br/&gt;&lt;b&gt;WARNING&lt;/b&gt;: features need to be ordered in the same way as in source_data.

        n_components: int, default to None
            Number of components. If not set here or in __init__, then use the maximum number of principal components
            possible for source and target.

        n_pv: int, default to None
            Number of Principal Vectors. If not set here or in __init__, then maximum number of PV will be computed.

        method : str, default to &#39;two-stage&#39;
            Method used for computing the principal vectors. Only &#39;two-stage&#39; has been implemented.

        step: int, default to 100
            Number of interpolation steps.

        with_interpolation: bool, default to True
            Bool indicating whether interpolation shall also be fitted. Useful for just computing PV
            prior to null distribution fitting (and choose of PV number).

        left_center: bool, default to True
            Bool indicating whether the output should be mean-centered, i.e. whether source and target
            consensus features values (or PVs if no interpolation) must have an independent mean-centering.

        Returns
        -------
        self : TRANSACT
            Fitted instance.
        &#34;&#34;&#34;

        # Save parameters
        self.source_data_ = source_data
        self.target_data_ = target_data
        self.method = method or self.method
        self.n_components = n_components or self.n_components
        self.n_pv = n_pv or self.n_pv
        self.step = step or self.step
        self.left_center = left_center

        # Compute kernel values
        self.kernel_values_.fit(source_data, target_data, center=False)

        # Compute principal vectors
        self.principal_vectors_ = PVComputation(self.kernel, self.kernel_params_)
        self.principal_vectors_.fit(self.source_data_,
                                    self.target_data_,
                                    method=self.method,
                                    n_components=self.n_components,
                                    n_pv=self.n_pv)

        # Stop here if interpolation should not be computed.
        if not with_interpolation:
            return self

        # Set up interpolation scheme
        self.interpolation_ = Interpolation(self.kernel, self.kernel_params_)
        self.interpolation_.fit(self.principal_vectors_, self.kernel_values_)

        # Compute optimal interpolation time
        self._compute_optimal_time(step=self.step, left_center=self.left_center)

        self.is_fitted = True

        return self


    def null_distribution_pv_similarity(self, method=&#39;gene_shuffling&#39;, n_iter=100):
        &#34;&#34;&#34;
        Generate a null distribution for the PV similarity function:
        &lt;ul&gt;
            &lt;li&gt; Gene shuffling: genes get shuffled in source to destroy any structure existing
            at the gene-level while preserving the sample structure. PV get recomputed and 
            similarity is saved.
        &lt;/ul&gt;

        Parameters
        ----------
        method : string, default to gene_shuffling
            Method used for generating the null distribution.
            Only method developped: gene_shuffling

        n_iter: int, default to 100
            Number of iterations

        Returns
        -------
        np.ndarray, dtype=float, shape (n_iter, n_pv)
            Array containing the distribution of similarity after shuffling. Each row
            contains the values of one shuffling across PVs.
        &#34;&#34;&#34;

        if method.lower() == &#39;gene_shuffling&#39;:
            null_method = self._gene_shuffling
        else:
            raise NotImplementedError(&#39;%s is not a proper method for generating null distribution&#39;%(method))

        null_distribution = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)\
                                    (delayed(null_method)() for _ in range(n_iter))

        return np.array(null_distribution)


    def _gene_shuffling(self):
        perm = np.random.permutation(self.source_data_.shape[1])
        pv = PVComputation(self.kernel, self.kernel_params_)
        pv.fit(self.source_data_[:,perm],
            self.target_data_,
            method=self.method,
            n_components=self.n_components,
            n_pv=self.n_pv)

        return np.cos(pv.canonical_angles)


    def fit_predictor(self, X, y, alpha_values=None, l1_ratio=0.5):
        &#34;&#34;&#34;
        Project X on consensus features and train a predictor of drug response.

        Parameters
        ----------
        X : np.ndarray of shape (n_samples, n_features), dtype=float
            Dataset to project. Features should be ordered in same way as in source_data
            and target_data.

        y : np.ndarray of shape (n_samples, 1), dtype=float
            Output to predict

        Returns
        -------
        &#34;&#34;&#34;
        self.alpha_values = alpha_values if alpha_values is not None else np.logspace(-10,5,34)
        self.l1_ratio_values = [0.2, 0.4, 0.5, 0.6, 0.8, 0.9, 1.]
        param_grid ={
            &#39;regression__alpha&#39;: self.alpha_values,
            &#39;regression__l1_ratio&#39;: self.l1_ratio_values
        }

        #Grid search setup
        self.predictive_clf = GridSearchCV(Pipeline([
                                (&#39;scaler&#39;, StandardScaler(with_mean=True, with_std=False)),
                                (&#39;regression&#39;, ElasticNet())
                                ]),\
                                cv=10,
                                n_jobs=self.n_jobs,
                                param_grid=param_grid,
                                verbose=self.verbose,
                                scoring=&#39;neg_mean_squared_error&#39;)
        self.predictive_clf.fit(self.transform(X, center=False), y)

        return self


    def compute_pred_performance(self, X, y, cv=10):
        &#34;&#34;&#34;
        Compute predictive performance of predictive model by cross-validation
        on X and y.

        Parameters
        ----------
        X : np.ndarray of shape (n_samples, n_features), dtype=float
            Dataset to project. Features should be ordered in same way as in source_data
            and target_data.

        Returns
        -------
        np.ndarray of shape (n_samples, n_pv), dtype=float
            Dataset projected on consensus features.
        &#34;&#34;&#34;

        kf = KFold(n_splits=cv, shuffle=True)
        X_projected = self.transform(X)

        if self.predictive_clf is None:
            print(&#39;BEWARE: NOT FITTED INSTANCE&#39;)
            self.fit_predictor(X,y)
        clf = clone(self.predictive_clf)

        y_predicted = np.zeros(X.shape[0])
        for train_index, test_index in kf.split(X_projected):
            clf.fit(X_projected[train_index], y[train_index])
            y_predicted[test_index] = clf.predict(X_projected[test_index])

        return scipy.stats.pearsonr(y_predicted, y)


    def predict(self, X):
        &#34;&#34;&#34;
        Predict the drug response of a set of samples, i.e.:
        &lt;ul&gt;
            &lt;li&gt; Project data on consensus features.
            &lt;li&gt; Use the Elastic Net model to predict based on the consensus features.
        &lt;/ul&gt;

        Parameters
        ----------
        X : np.ndarray, dtype=float
            Dataset to project, of shape (n_samples, n_features). Features should be ordered in same way as
            in source_data and target_data.

        Returns
        -------
        np.ndarray of shape (n_samples, 1), dtype=float
            Predicted drug response values.
        &#34;&#34;&#34;
        return self.predictive_clf.predict(self.transform(X, center=False))


    def transform(self, X, center=False):
        &#34;&#34;&#34;
        Project a dataset X onto the consensus features.

        Parameters
        ----------
        X : np.ndarray, dtype=float
            Dataset to project, of shape (n_samples, n_features). Features should be ordered in same way as
            in source_data and target_data.

        Returns
        -------
        np.ndarray of shape (n_samples, n_pv), dtype=float
            Dataset projected on consensus features.
        &#34;&#34;&#34;
        return self.interpolation_.transform(X, self.optimal_time, center)


    def _compute_optimal_time(self, step=100, left_center=True):
        # Based on Kolmogorov Smirnov statistics, find interpolation time

        # Compute the interpolated values
        interpolated_values = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)\
                            (delayed(self.interpolation_.project_data)(s/step, center=left_center)
                                for s in range(step+1))
        interpolated_values = np.array(interpolated_values).transpose(2,0,1)
        source_interpolated_values = interpolated_values[:,:,:self.source_data_.shape[0]]
        target_interpolated_values = interpolated_values[:,:,self.source_data_.shape[0]:]

        self.optimal_time = []
        self.ks_statistics = []
        self.ks_p_values = []

        # For each PV, find the time when interpolation has the largest overlap.
        for source_pv, target_pv in zip(source_interpolated_values, target_interpolated_values):
            self.ks_statistics.append([])
            for s, t in zip(source_pv, target_pv):
                self.ks_statistics[-1].append(scipy.stats.ks_2samp(s,t))
            self.ks_statistics[-1] = list(zip(*self.ks_statistics[-1]))
            self.ks_p_values.append(self.ks_statistics[-1][-1])
            self.ks_statistics[-1] = self.ks_statistics[-1][0]
            self.optimal_time.append(np.argmin(self.ks_statistics[-1])/step)

        # Save the different statistics
        self.optimal_time = np.array(self.optimal_time) # Optimal tau for each PV.
        self.ks_statistics = np.array(self.ks_statistics) # Computed KS statistics between each PV.
        self.ks_p_values = np.array(self.ks_p_values) # Corresponding p_values.</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="transact.TRANSACT.TRANSACT.compute_pred_performance"><code class="name flex">
<span>def <span class="ident">compute_pred_performance</span></span>(<span>self, X, y, cv=10)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute predictive performance of predictive model by cross-validation
on X and y.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code> of <code>shape (n_samples, n_features), dtype=float</code></dt>
<dd>Dataset to project. Features should be ordered in same way as in source_data
and target_data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code> of <code>shape (n_samples, n_pv), dtype=float</code></dt>
<dd>Dataset projected on consensus features.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_pred_performance(self, X, y, cv=10):
    &#34;&#34;&#34;
    Compute predictive performance of predictive model by cross-validation
    on X and y.

    Parameters
    ----------
    X : np.ndarray of shape (n_samples, n_features), dtype=float
        Dataset to project. Features should be ordered in same way as in source_data
        and target_data.

    Returns
    -------
    np.ndarray of shape (n_samples, n_pv), dtype=float
        Dataset projected on consensus features.
    &#34;&#34;&#34;

    kf = KFold(n_splits=cv, shuffle=True)
    X_projected = self.transform(X)

    if self.predictive_clf is None:
        print(&#39;BEWARE: NOT FITTED INSTANCE&#39;)
        self.fit_predictor(X,y)
    clf = clone(self.predictive_clf)

    y_predicted = np.zeros(X.shape[0])
    for train_index, test_index in kf.split(X_projected):
        clf.fit(X_projected[train_index], y[train_index])
        y_predicted[test_index] = clf.predict(X_projected[test_index])

    return scipy.stats.pearsonr(y_predicted, y)</code></pre>
</details>
</dd>
<dt id="transact.TRANSACT.TRANSACT.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, source_data, target_data, n_components=None, n_pv=None, method='two-stage', step=100, with_interpolation=True, left_center=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Consensus Features (CFs) onto which predictive models can be trained.
<br/> Specifically:</p>
<ul>
<li> Compute the kernel matrices.
<li> Compute the cosine similarity matrix.
<li> Compute principal vectors.
<li> Interpolate between the PVs.
<li> Find optimal interpolation time.
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>source_data</code></strong> :&ensp;<code>np.ndarray, dtype=float</code></dt>
<dd>Source data, matrix with samples in the rows, i.e. shape (n_source_samples, n_features).
<br./> pandas.DataFrame are supported.</dd>
<dt><strong><code>target_data</code></strong> :&ensp;<code>np.ndarray, dtype=float</code></dt>
<dd>Source data, matrix with samples in the rows, i.e. shape (n_target_samples, n_features).
<br./> pandas.DataFrame are supported.
<br/><b>WARNING</b>: features need to be ordered in the same way as in source_data.</dd>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int</code>, default <code>to None</code></dt>
<dd>Number of components. If not set here or in <strong>init</strong>, then use the maximum number of principal components
possible for source and target.</dd>
<dt><strong><code>n_pv</code></strong> :&ensp;<code>int</code>, default <code>to None</code></dt>
<dd>Number of Principal Vectors. If not set here or in <strong>init</strong>, then maximum number of PV will be computed.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code>, default <code>to 'two-stage'</code></dt>
<dd>Method used for computing the principal vectors. Only 'two-stage' has been implemented.</dd>
<dt><strong><code>step</code></strong> :&ensp;<code>int</code>, default <code>to 100</code></dt>
<dd>Number of interpolation steps.</dd>
<dt><strong><code>with_interpolation</code></strong> :&ensp;<code>bool</code>, default <code>to True</code></dt>
<dd>Bool indicating whether interpolation shall also be fitted. Useful for just computing PV
prior to null distribution fitting (and choose of PV number).</dd>
<dt><strong><code>left_center</code></strong> :&ensp;<code>bool</code>, default <code>to True</code></dt>
<dd>Bool indicating whether the output should be mean-centered, i.e. whether source and target
consensus features values (or PVs if no interpolation) must have an independent mean-centering.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code><a title="transact.TRANSACT.TRANSACT" href="#transact.TRANSACT.TRANSACT">TRANSACT</a></code></dt>
<dd>Fitted instance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self,
        source_data,
        target_data,
        n_components=None,
        n_pv=None,
        method=&#39;two-stage&#39;,
        step=100,
        with_interpolation=True,
        left_center=True):

    &#34;&#34;&#34;
    Compute the Consensus Features (CFs) onto which predictive models can be trained.
    &lt;br/&gt; Specifically:
    &lt;ul&gt;
        &lt;li&gt; Compute the kernel matrices.
        &lt;li&gt; Compute the cosine similarity matrix.
        &lt;li&gt; Compute principal vectors.
        &lt;li&gt; Interpolate between the PVs.
        &lt;li&gt; Find optimal interpolation time.
    &lt;/ul&gt;

    Parameters
    ----------
    source_data : np.ndarray, dtype=float
        Source data, matrix with samples in the rows, i.e. shape (n_source_samples, n_features).
        &lt;br./&gt; pandas.DataFrame are supported.

    target_data : np.ndarray, dtype=float
        Source data, matrix with samples in the rows, i.e. shape (n_target_samples, n_features).
        &lt;br./&gt; pandas.DataFrame are supported.
        &lt;br/&gt;&lt;b&gt;WARNING&lt;/b&gt;: features need to be ordered in the same way as in source_data.

    n_components: int, default to None
        Number of components. If not set here or in __init__, then use the maximum number of principal components
        possible for source and target.

    n_pv: int, default to None
        Number of Principal Vectors. If not set here or in __init__, then maximum number of PV will be computed.

    method : str, default to &#39;two-stage&#39;
        Method used for computing the principal vectors. Only &#39;two-stage&#39; has been implemented.

    step: int, default to 100
        Number of interpolation steps.

    with_interpolation: bool, default to True
        Bool indicating whether interpolation shall also be fitted. Useful for just computing PV
        prior to null distribution fitting (and choose of PV number).

    left_center: bool, default to True
        Bool indicating whether the output should be mean-centered, i.e. whether source and target
        consensus features values (or PVs if no interpolation) must have an independent mean-centering.

    Returns
    -------
    self : TRANSACT
        Fitted instance.
    &#34;&#34;&#34;

    # Save parameters
    self.source_data_ = source_data
    self.target_data_ = target_data
    self.method = method or self.method
    self.n_components = n_components or self.n_components
    self.n_pv = n_pv or self.n_pv
    self.step = step or self.step
    self.left_center = left_center

    # Compute kernel values
    self.kernel_values_.fit(source_data, target_data, center=False)

    # Compute principal vectors
    self.principal_vectors_ = PVComputation(self.kernel, self.kernel_params_)
    self.principal_vectors_.fit(self.source_data_,
                                self.target_data_,
                                method=self.method,
                                n_components=self.n_components,
                                n_pv=self.n_pv)

    # Stop here if interpolation should not be computed.
    if not with_interpolation:
        return self

    # Set up interpolation scheme
    self.interpolation_ = Interpolation(self.kernel, self.kernel_params_)
    self.interpolation_.fit(self.principal_vectors_, self.kernel_values_)

    # Compute optimal interpolation time
    self._compute_optimal_time(step=self.step, left_center=self.left_center)

    self.is_fitted = True

    return self</code></pre>
</details>
</dd>
<dt id="transact.TRANSACT.TRANSACT.fit_predictor"><code class="name flex">
<span>def <span class="ident">fit_predictor</span></span>(<span>self, X, y, alpha_values=None, l1_ratio=0.5)</span>
</code></dt>
<dd>
<div class="desc"><p>Project X on consensus features and train a predictor of drug response.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code> of <code>shape (n_samples, n_features), dtype=float</code></dt>
<dd>Dataset to project. Features should be ordered in same way as in source_data
and target_data.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code> of <code>shape (n_samples, 1), dtype=float</code></dt>
<dd>Output to predict</dd>
</dl>
<h2 id="returns">Returns</h2></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_predictor(self, X, y, alpha_values=None, l1_ratio=0.5):
    &#34;&#34;&#34;
    Project X on consensus features and train a predictor of drug response.

    Parameters
    ----------
    X : np.ndarray of shape (n_samples, n_features), dtype=float
        Dataset to project. Features should be ordered in same way as in source_data
        and target_data.

    y : np.ndarray of shape (n_samples, 1), dtype=float
        Output to predict

    Returns
    -------
    &#34;&#34;&#34;
    self.alpha_values = alpha_values if alpha_values is not None else np.logspace(-10,5,34)
    self.l1_ratio_values = [0.2, 0.4, 0.5, 0.6, 0.8, 0.9, 1.]
    param_grid ={
        &#39;regression__alpha&#39;: self.alpha_values,
        &#39;regression__l1_ratio&#39;: self.l1_ratio_values
    }

    #Grid search setup
    self.predictive_clf = GridSearchCV(Pipeline([
                            (&#39;scaler&#39;, StandardScaler(with_mean=True, with_std=False)),
                            (&#39;regression&#39;, ElasticNet())
                            ]),\
                            cv=10,
                            n_jobs=self.n_jobs,
                            param_grid=param_grid,
                            verbose=self.verbose,
                            scoring=&#39;neg_mean_squared_error&#39;)
    self.predictive_clf.fit(self.transform(X, center=False), y)

    return self</code></pre>
</details>
</dd>
<dt id="transact.TRANSACT.TRANSACT.null_distribution_pv_similarity"><code class="name flex">
<span>def <span class="ident">null_distribution_pv_similarity</span></span>(<span>self, method='gene_shuffling', n_iter=100)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a null distribution for the PV similarity function:</p>
<ul>
<li> Gene shuffling: genes get shuffled in source to destroy any structure existing
at the gene-level while preserving the sample structure. PV get recomputed and
similarity is saved.
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>method</code></strong> :&ensp;<code>string</code>, default <code>to gene_shuffling</code></dt>
<dd>Method used for generating the null distribution.
Only method developped: gene_shuffling</dd>
<dt><strong><code>n_iter</code></strong> :&ensp;<code>int</code>, default <code>to 100</code></dt>
<dd>Number of iterations</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray, dtype=float, shape (n_iter, n_pv)</code></dt>
<dd>Array containing the distribution of similarity after shuffling. Each row
contains the values of one shuffling across PVs.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def null_distribution_pv_similarity(self, method=&#39;gene_shuffling&#39;, n_iter=100):
    &#34;&#34;&#34;
    Generate a null distribution for the PV similarity function:
    &lt;ul&gt;
        &lt;li&gt; Gene shuffling: genes get shuffled in source to destroy any structure existing
        at the gene-level while preserving the sample structure. PV get recomputed and 
        similarity is saved.
    &lt;/ul&gt;

    Parameters
    ----------
    method : string, default to gene_shuffling
        Method used for generating the null distribution.
        Only method developped: gene_shuffling

    n_iter: int, default to 100
        Number of iterations

    Returns
    -------
    np.ndarray, dtype=float, shape (n_iter, n_pv)
        Array containing the distribution of similarity after shuffling. Each row
        contains the values of one shuffling across PVs.
    &#34;&#34;&#34;

    if method.lower() == &#39;gene_shuffling&#39;:
        null_method = self._gene_shuffling
    else:
        raise NotImplementedError(&#39;%s is not a proper method for generating null distribution&#39;%(method))

    null_distribution = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)\
                                (delayed(null_method)() for _ in range(n_iter))

    return np.array(null_distribution)</code></pre>
</details>
</dd>
<dt id="transact.TRANSACT.TRANSACT.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict the drug response of a set of samples, i.e.:</p>
<ul>
<li> Project data on consensus features.
<li> Use the Elastic Net model to predict based on the consensus features.
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray, dtype=float</code></dt>
<dd>Dataset to project, of shape (n_samples, n_features). Features should be ordered in same way as
in source_data and target_data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code> of <code>shape (n_samples, 1), dtype=float</code></dt>
<dd>Predicted drug response values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#34;&#34;&#34;
    Predict the drug response of a set of samples, i.e.:
    &lt;ul&gt;
        &lt;li&gt; Project data on consensus features.
        &lt;li&gt; Use the Elastic Net model to predict based on the consensus features.
    &lt;/ul&gt;

    Parameters
    ----------
    X : np.ndarray, dtype=float
        Dataset to project, of shape (n_samples, n_features). Features should be ordered in same way as
        in source_data and target_data.

    Returns
    -------
    np.ndarray of shape (n_samples, 1), dtype=float
        Predicted drug response values.
    &#34;&#34;&#34;
    return self.predictive_clf.predict(self.transform(X, center=False))</code></pre>
</details>
</dd>
<dt id="transact.TRANSACT.TRANSACT.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X, center=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Project a dataset X onto the consensus features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray, dtype=float</code></dt>
<dd>Dataset to project, of shape (n_samples, n_features). Features should be ordered in same way as
in source_data and target_data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code> of <code>shape (n_samples, n_pv), dtype=float</code></dt>
<dd>Dataset projected on consensus features.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X, center=False):
    &#34;&#34;&#34;
    Project a dataset X onto the consensus features.

    Parameters
    ----------
    X : np.ndarray, dtype=float
        Dataset to project, of shape (n_samples, n_features). Features should be ordered in same way as
        in source_data and target_data.

    Returns
    -------
    np.ndarray of shape (n_samples, n_pv), dtype=float
        Dataset projected on consensus features.
    &#34;&#34;&#34;
    return self.interpolation_.transform(X, self.optimal_time, center)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#example">Example</a></li>
<li><a href="#notes">Notes</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="transact" href="index.html">transact</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="transact.TRANSACT.TRANSACT" href="#transact.TRANSACT.TRANSACT">TRANSACT</a></code></h4>
<ul class="">
<li><code><a title="transact.TRANSACT.TRANSACT.compute_pred_performance" href="#transact.TRANSACT.TRANSACT.compute_pred_performance">compute_pred_performance</a></code></li>
<li><code><a title="transact.TRANSACT.TRANSACT.fit" href="#transact.TRANSACT.TRANSACT.fit">fit</a></code></li>
<li><code><a title="transact.TRANSACT.TRANSACT.fit_predictor" href="#transact.TRANSACT.TRANSACT.fit_predictor">fit_predictor</a></code></li>
<li><code><a title="transact.TRANSACT.TRANSACT.null_distribution_pv_similarity" href="#transact.TRANSACT.TRANSACT.null_distribution_pv_similarity">null_distribution_pv_similarity</a></code></li>
<li><code><a title="transact.TRANSACT.TRANSACT.predict" href="#transact.TRANSACT.TRANSACT.predict">predict</a></code></li>
<li><code><a title="transact.TRANSACT.TRANSACT.transform" href="#transact.TRANSACT.TRANSACT.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>